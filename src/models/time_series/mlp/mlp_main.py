"""
MLP Main Module

This module contains the main function and MLflow integration for standalone MLP execution.
Includes the complete pipeline for hypertuning and evaluation.
"""

import torch
import torch.nn as nn
import pandas as pd
import mlflow
import mlflow.pytorch
import optuna
import pickle
import os
from datetime import datetime
from typing import Dict, Any

from src.models.time_series.mlp.mlp_predictor import MLPPredictor
from src.utils.logger import get_logger
from src.utils.mlflow_utils import MLFlowManager
from src.data_utils.ml_data_pipeline import prepare_ml_data_for_training_with_cleaning
from src.models.time_series.mlp.mlp_evaluation import MLPEvaluationMixin
from src.models.time_series.mlp.mlp_architecture import MLPDataUtils
from src.models.time_series.mlp.mlp_optimization import MLPOptimizationMixin

logger = get_logger(__name__)
experiment_name = "mlp_stock_predictor"
class MLPWrapper(nn.Module):
    """
    PyTorch Module wrapper for MLPPredictorWithMLflow to enable MLflow compatibility.
    
    This wrapper preserves all functionality of the original predictor while making it
    compatible with MLflow's PyTorch model logging requirements.
    """
    
    def __init__(self, predictor):
        """
        Initialize the wrapper with a MLPPredictorWithMLflow instance.
        
        Args:
            predictor: MLPPredictorWithMLflow instance to wrap
        """
        super().__init__()
        self.predictor = predictor
        
        # Extract the actual PyTorch model for forward pass
        if hasattr(predictor, 'model') and predictor.model is not None:
            self.model = predictor.model
        else:
            raise ValueError("Predictor must have a trained model")
        
        # Preserve all predictor attributes for compatibility
        self.scaler = getattr(predictor, 'scaler', None)
        self.feature_names = getattr(predictor, 'feature_names', [])
        self.config = getattr(predictor, 'config', {})
        self.device = getattr(predictor, 'device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
        self.model_name = getattr(predictor, 'model_name', 'MLP')
        self.threshold_evaluator = getattr(predictor, 'threshold_evaluator', None)
        self.is_trained = getattr(predictor, 'is_trained', False)
        
        # Preserve threshold optimization attributes
        self.optimal_threshold = getattr(predictor, 'optimal_threshold', 0.5)
        self.confidence_method = getattr(predictor, 'confidence_method', 'variance')
        self.best_threshold_info = getattr(predictor, 'best_threshold_info', {})
        
        logger.info("‚úÖ MLPWrapper initialized with all predictor attributes preserved")
    
    def forward(self, x):
        """
        Forward pass through the PyTorch model.
        
        Args:
            x: Input tensor
            
        Returns:
            Model predictions
        """
        return self.model(x)
    
    def predict(self, features_df):
        """
        Make predictions using the wrapped predictor's logic.
        
        Args:
            features_df: Input features DataFrame
            
        Returns:
            Predictions as numpy array
        """
        return self.predictor.predict(features_df)
    
    def get_prediction_confidence(self, features_df, method='variance'):
        """
        Get prediction confidence scores using the wrapped predictor's logic.
        
        Args:
            features_df: Input features DataFrame
            method: Confidence calculation method
            
        Returns:
            Confidence scores as numpy array
        """
        return self.predictor.get_prediction_confidence(features_df, method)
    
    def predict_with_threshold(self, X, return_confidence=False, threshold=None, confidence_method=None):
        """
        Make predictions with confidence-based filtering.
        
        Args:
            X: Feature matrix
            return_confidence: Whether to return confidence scores
            threshold: Confidence threshold
            confidence_method: Confidence method
            
        Returns:
            Dictionary with predictions and filtering info
        """
        return self.predictor.predict_with_threshold(X, return_confidence, threshold, confidence_method)
    
    def get_prediction_with_confidence(self, features_df, confidence_method='variance'):
        """
        Get predictions with confidence scores.
        
        Args:
            features_df: Input features DataFrame
            confidence_method: Confidence calculation method
            
        Returns:
            Tuple of (predictions, confidence_scores)
        """
        predictions = self.predict(features_df)
        confidence_scores = self.get_prediction_confidence(features_df, confidence_method)
        return predictions, confidence_scores


class MLPPredictorWithMLflow(MLPPredictor, MLPEvaluationMixin, MLPOptimizationMixin):
    """
    MLPPredictor with MLflow integration capabilities.
    """
    
    def save_model(self, metrics: Dict[str, float], params: Dict[str, Any], 
                    X_eval: pd.DataFrame, experiment_name: str = None, scaler=None) -> str:
        """
        Save the trained MLP model to MLflow with all necessary components
        
        Args:
            metrics: Model evaluation metrics to log
            params: Model parameters to log
            X_eval: Evaluation features for signature generation
            experiment_name: Experiment name (uses default if None)
            scaler: Fitted StandardScaler instance to save with model
            
        Returns:
            MLflow run ID where the model was saved
        """
        if self.model is None:
            raise RuntimeError("No trained model to save")
        
        if experiment_name is None:
            experiment_name = "mlp_stock_predictor"
        
        try:
            # End any existing run before starting a new one
            try:
                mlflow.end_run()
                logger.info("Ended existing MLflow run before starting new one")
            except Exception as e:
                logger.info(f"No existing run to end: {e}")
            
            # Set up MLflow tracking
            mlflow.set_experiment(experiment_name)

            # Start a new run
            with mlflow.start_run(
                run_name=f"mlp_final_{datetime.now().strftime('%Y%m%d_%H%M')}"
            ) as run:
                # Log parameters (excluding base params if desired)
                params_to_log = {
                    k: v
                    for k, v in params.items()
                    if k not in ["device", "objective", "verbosity", "seed", "nthread", "verbose"]
                }
                mlflow.log_params(params_to_log)
                mlflow.log_metrics(metrics)

                if scaler is not None:
                    try:
                        # Create temporary directory and save scaler with specific name
                        temp_dir = "src/models/scalers"
                        os.makedirs(temp_dir, exist_ok=True)
                        scaler_path = os.path.join(temp_dir, 'scaler.pkl')
                        
                        with open(scaler_path, 'wb') as f:
                            pickle.dump(scaler, f)
                        
                        mlflow.log_artifact(scaler_path, artifact_path="scaler")
                        
                        # Clean up temp files
                        os.unlink(scaler_path)
                        logger.info("‚úÖ Feature scaler saved to MLflow artifacts")
                    except Exception as scaler_error:
                        logger.warning(f"‚ö†Ô∏è Could not save scaler: {str(scaler_error)}")
                else:
                    logger.info("‚ÑπÔ∏è No scaler provided - model will use raw features for prediction")
                
                # Create input example using the DataFrame X_eval
                input_example = X_eval.iloc[:5].copy()

                # Identify and convert integer columns to float64
                if hasattr(input_example, "dtypes"):
                    for col in input_example.columns:
                        if input_example[col].dtype.kind == "i":
                            logger.info(f"Converting integer column '{col}' to float64 for signature")
                            input_example[col] = input_example[col].astype("float64")
                
                # Create wrapper for MLflow compatibility
                wrapper = MLPWrapper(self)
                
                # Infer signature using the wrapper's predict method
                try:
                    predictions_example = wrapper.predict(input_example)
                    logger.info("‚úÖ Signature generated using wrapper predict method")
                    
                except Exception as predict_error:
                    logger.warning(f"‚ö†Ô∏è Could not use wrapper predict method for signature: {str(predict_error)}")
                    logger.info("üîÑ Falling back to direct model inference for signature generation")
                    
                    # Fallback to direct model inference
                    wrapper.eval()
                    with torch.no_grad():
                        input_tensor = torch.FloatTensor(input_example.values)
                        device = next(wrapper.parameters()).device
                        input_tensor = input_tensor.to(device)
                        
                        if input_tensor.device != device:
                            logger.warning(f"‚ö†Ô∏è Device mismatch detected: input_tensor on {input_tensor.device}, model on {device}")
                            input_tensor = input_tensor.to(device)
                        
                        predictions_example = wrapper(input_tensor).cpu().numpy()
                
                signature = mlflow.models.infer_signature(input_example, predictions_example)
                
                # Log the wrapper instead of the predictor
                try:
                    mlflow.pytorch.log_model(
                        pytorch_model=wrapper,
                        name="model",
                        signature=signature,
                    )
                    logger.info("‚úÖ MLP Wrapper successfully logged to MLflow")
                except RuntimeError as model_error:
                    if "device" in str(model_error).lower():
                        logger.error(f"‚ùå Device mismatch error during model logging: {str(model_error)}")
                        logger.info("üîÑ Attempting to move model to CPU for logging...")
                        try:
                            # Create a temporary copy for CPU logging
                            wrapper_cpu = wrapper.cpu()
                            
                            mlflow.pytorch.log_model(
                                pytorch_model=wrapper_cpu,
                                name="model",
                                signature=signature,
                            )
                            wrapper.to(self.device)
                            logger.info("‚úÖ Model successfully logged to MLflow (CPU fallback)")
                        except Exception as cpu_error:
                            logger.error(f"‚ùå Failed to log model even with CPU fallback: {str(cpu_error)}")
                            raise
                    else:
                        logger.error(f"‚ùå Model logging failed: {str(model_error)}")
                        raise
                except Exception as e:
                    logger.error(f"‚ùå Unexpected error during model logging: {str(e)}")
                    raise
                
                logger.info(f"‚úÖ MLP model saved successfully. Run ID: {run.info.run_id}")
                return run.info.run_id

        except Exception as e:
            logger.error(f"Error saving model to MLflow: {str(e)}")
            return None

    def load_model(self, run_id: str, experiment_name: str = None, model_id: str = None) -> bool:
        """
        Load a trained MLP model from MLflow based on the given run ID or model ID
        
        Args:
            run_id: MLflow run ID to load the model from (optional if model_id provided)
            experiment_name: Experiment name (uses default if None)
            model_id: Optional model ID for specific model loading (preferred over run_id)
            
        Returns:
            bool: True if model loaded successfully, False otherwise
        """
        try:
            if experiment_name is None:
                experiment_name = "mlp_stock_predictor"
            
            # Set the experiment
            mlflow.set_experiment(experiment_name)
            
            # Get MLflow client for artifact access
            client = mlflow.tracking.MlflowClient()
            
            # Load scaler from run artifacts if run_id is provided
            scaler_loaded = False
            if run_id:
                scaler_loaded = self._load_scaler_from_run(client, run_id)
            
            # Try to load model using model_id first (preferred method)
            model_loaded = False
            if model_id:
                try:
                    logger.info(f"üéØ Loading model using specific model ID: {model_id}")
                    
                    # Get the logged model directly using mlflow.get_logged_model()
                    logged_model_info = mlflow.get_logged_model(model_id)
                    logger.info(f"‚úÖ Retrieved logged model: {logged_model_info.name}")

                    loaded_wrapper = mlflow.pytorch.load_model(logged_model_info.model_uri)
                    
                    # Check if loaded model is a wrapper
                    if hasattr(loaded_wrapper, 'predictor'):
                        # It's a wrapper - extract components
                        self.model = loaded_wrapper.model
                        self.scaler = loaded_wrapper.scaler
                        self.feature_names = loaded_wrapper.feature_names
                        self.config.update(loaded_wrapper.config)
                        self.device = loaded_wrapper.device
                        self.model_name = loaded_wrapper.model_name
                        self.threshold_evaluator = loaded_wrapper.threshold_evaluator
                        self.is_trained = loaded_wrapper.is_trained
                        self.optimal_threshold = loaded_wrapper.optimal_threshold
                        self.confidence_method = loaded_wrapper.confidence_method
                        self.best_threshold_info = loaded_wrapper.best_threshold_info
                        logger.info("‚úÖ Model wrapper loaded successfully with all components")
                    else:
                        # It's a raw model - use as is
                        self.model = loaded_wrapper
                        logger.info("‚úÖ Raw model loaded (no wrapper components)")
                    
                    self.model.to(self.device)
                    model_loaded = True
                    
                    logger.info("‚úÖ Model loaded successfully from logged model")
                    
                    # Extract feature names from model signature if not already loaded
                    if not hasattr(self, 'feature_names') or not self.feature_names:
                        self._extract_feature_names_from_model(logged_model_info.model_uri, "logged model")
                    
                except Exception as model_error:
                    logger.error(f"‚ùå Failed to load model using model_id {model_id}: {str(model_error)}")
                    return False
            
            if not model_loaded:
                logger.error("‚ùå Failed to load model from all available sources")
                return False
            
            # Mark as trained
            self.is_trained = True
            
            logger.info("‚úÖ Model loaded successfully")
            if model_id:
                logger.info(f"   Model ID: {model_id}")
            if run_id:
                logger.info(f"   Run ID: {run_id}")
            logger.info(f"   Experiment: {experiment_name}")
            logger.info(f"   Scaler loaded: {scaler_loaded}")
            logger.info(f"   Feature names loaded: {len(self.feature_names) if hasattr(self, 'feature_names') and self.feature_names else 0}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading model: {str(e)}")
            return False

    def _load_scaler_from_run(self, client, run_id: str) -> bool:
        """
        Load scaler from run artifacts
        
        Args:
            client: MLflow client
            run_id: Run ID to load scaler from
            
        Returns:
            bool: True if scaler loaded successfully, False otherwise
        """
        try:
            artifacts = client.list_artifacts(run_id)
            logger.info(f"üì¶ Found {len(artifacts)} artifacts in run {run_id}")
            for artifact in artifacts:
                logger.info(f"   - {artifact.path}")
            
            # Find scaler artifact
            scaler_artifact_path = None
            for artifact in artifacts:
                if artifact.path == "scaler" or artifact.path.endswith("scaler.pkl"):
                    scaler_artifact_path = artifact.path
                    break
            
            if scaler_artifact_path:
                try:
                    # Download scaler artifact
                    local_path = "src/models/scalers"
                    client.download_artifacts(run_id, scaler_artifact_path, dst_path=local_path)
                    scaler_local_path = os.path.join(local_path, "scaler", "scaler.pkl")
                    with open(scaler_local_path, 'rb') as f:
                        self.scaler = pickle.load(f)
                    
                    logger.info("‚úÖ Feature scaler loaded from MLflow artifacts")
                    
                    # Clean up downloaded file
                    os.unlink(scaler_local_path)
                    
                    return True
                    
                except Exception as scaler_error:
                    logger.warning(f"‚ö†Ô∏è Could not load scaler: {str(scaler_error)}")
                    return False
            else:
                logger.info("‚ÑπÔ∏è No scaler artifact found - model will use raw features for prediction")
                return False
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error listing artifacts for scaler loading: {str(e)}")
            return False

    def _extract_feature_names_from_model(self, model_uri: str, source: str):
        """
        Extract feature names from model signature
        
        Args:
            model_uri: Model URI to extract signature from
            source: Source description for logging
        """
        feature_names_from_signature = []
        try:
            # Direct approach: get signature from model URI
            logger.info(f"Attempting to load model signature from: {model_uri}")
            
            # Load model info to get signature
            from mlflow.models import get_model_info
            model_info = get_model_info(model_uri)
            logger.info(f"Model info loaded: {model_info is not None}")
            
            if model_info and model_info.signature:
                logger.info(f"Model signature found: {model_info.signature is not None}")
                
                if model_info.signature.inputs:
                    logger.info(f"Signature inputs found: {len(model_info.signature.inputs.inputs) if hasattr(model_info.signature.inputs, 'inputs') else 'No inputs attr'}")
                    
                    # Extract feature names from signature inputs
                    # Handle different signature input formats
                    if hasattr(model_info.signature.inputs, 'inputs'):
                        # Schema format
                        for input_spec in model_info.signature.inputs.inputs:
                            if hasattr(input_spec, 'name') and input_spec.name:
                                feature_names_from_signature.append(input_spec.name)
                    elif hasattr(model_info.signature.inputs, 'schema'):
                        # Alternative schema format
                        if hasattr(model_info.signature.inputs.schema, 'input_names'):
                            feature_names_from_signature = model_info.signature.inputs.schema.input_names
                    else:
                        logger.info(f"Signature inputs type: {type(model_info.signature.inputs)}")
                        logger.info(f"Signature inputs attributes: {dir(model_info.signature.inputs)}")
                    
                    if feature_names_from_signature:
                        self.feature_names = feature_names_from_signature
                        logger.info(f"‚úÖ Loaded {len(self.feature_names)} feature names from {source} signature")
                        logger.info(f"First 10 features: {self.feature_names[:10]}")
                    else:
                        logger.warning(f"‚ö†Ô∏è No feature names found in {source} signature inputs")
                else:
                    logger.warning(f"‚ö†Ô∏è {source} signature has no inputs")
            else:
                logger.warning(f"‚ö†Ô∏è No model signature found in {source} model info")
                    
        except Exception as signature_error:
            logger.warning(f"‚ö†Ô∏è Could not extract feature names from {source} signature: {str(signature_error)}")
            logger.info(f"Signature error details: {type(signature_error).__name__}: {signature_error}")
            
            # Fallback: Try to extract feature names from the loaded model's input layer
            try:
                logger.info(f"üîÑ Attempting fallback: extracting feature names from {source} model architecture...")
                if hasattr(self.model, 'input_size'):
                    # If the model has an input_size attribute, we can infer the number of features
                    input_size = self.model.input_size
                    logger.info(f"Model input size: {input_size}")
                    # Note: We can't get feature names from architecture, but we can at least validate the size
                    logger.warning(f"‚ö†Ô∏è Cannot extract feature names from {source} model architecture - will need to be provided externally")
            except Exception as fallback_error:
                logger.warning(f"‚ö†Ô∏è Fallback feature extraction for {source} also failed: {str(fallback_error)}")


def main():
    """
    Standalone MLP hypertuning and evaluation using load_all_data
    """
    logger.info("=" * 80)
    logger.info("üéØ STANDALONE MLP HYPERTUNING & EVALUATION")
    logger.info("=" * 80)
    
    try:
        # 0. Setup MLflow experiment tracking
        logger.info("0. Setting up MLflow experiment tracking...")
        mlflow_manager = MLFlowManager()
        experiment_id = mlflow_manager.setup_experiment(experiment_name)
        
        logger.info(f"‚úÖ MLflow experiment setup completed: {experiment_id}")
        logger.info(f"‚úÖ MLflow tracking URI: {mlflow.get_tracking_uri()}")
        
        # Define prediction horizon
        prediction_horizon = 10
        number_of_trials = 40
        # n_features_to_select = 80
        
        # OPTION 1: Use the enhanced data preparation function with cleaning (direct import)
        data_result = prepare_ml_data_for_training_with_cleaning(
            prediction_horizon=prediction_horizon,
            split_date='2025-02-01',
            ticker=None, 
            clean_features=True,  
            use_cache=True, 
        )
        
        # Extract prepared data
        X_train = data_result['X_train']
        X_test = data_result['X_test']
        y_train = data_result['y_train']
        y_test = data_result['y_test']
        target_column = data_result['target_column']
        train_date_range = data_result['train_date_range']
        test_date_range = data_result['test_date_range']

        logger.info(f"‚úÖ Data loaded successfully y_test range: {y_test.min()} to {y_test.max()} avg: {y_test.mean()}")
        
        # Remove outliers from target variables to improve prediction quality
        logger.info("üßπ Removing outliers from target variables...")
        
        # Calculate percentiles for outlier removal (keep middle 95%)
        y_train_lower = y_train.quantile(0.05)
        y_train_upper = y_train.quantile(0.95)
        
        # Create outlier masks
        y_train_outlier_mask = (y_train >= y_train_lower) & (y_train <= y_train_upper)
        
        # Apply outlier removal
        X_train_clean_outliers = X_train[y_train_outlier_mask]
        y_train_clean = y_train[y_train_outlier_mask]
        
        # Log outlier removal results
        logger.info("üìä Target outlier removal results:")
        logger.info(f"   Training: {len(y_train)} ‚Üí {len(y_train_clean)} samples ({len(y_train) - len(y_train_clean)} outliers removed)")
        logger.info(f"   Training target range: {y_train_clean.min():.6f} to {y_train_clean.max():.6f} (avg: {y_train_clean.mean():.6f})")
        
        # Update data with cleaned versions
        X_train = X_train_clean_outliers
        y_train = y_train_clean
        
        # Validate and clean the data
        X_train_clean= MLPDataUtils.validate_and_clean_data(X_train)
        X_test_clean = MLPDataUtils.validate_and_clean_data(X_test)
        
        # Extract cleaned data
        X_train = X_train_clean
        X_test = X_test_clean
        
        # Create SINGLE MLPPredictorWithMLflow instance for entire pipeline
        mlp_model = MLPPredictorWithMLflow(
            model_name="mlp_complete_pipeline",
            config={'input_size': len(X_train.columns)}
        )

        # 2. Perform feature selection using the same instance
        # selected_features = mlp_model.select_features(X_train_scaled, y_train, n_features_to_select)
        numerical_features = []
        for col in X_train.columns:
            if X_train[col].dtype in ['float64', 'float32', 'int64', 'int32', 'float', 'int']:
                numerical_features.append(col)
        
        selected_features = numerical_features
        logger.info(f"‚úÖ Using {len(selected_features)} numerical features - no feature selection applied")
        # Create new DataFrames with only the selected features
        X_train_selected = X_train[selected_features]
        X_test_selected = X_test[selected_features]
        logger.info(f"   DataFrames updated with {len(selected_features)} selected features.")
        
        # Update the model's input size for the selected features
        mlp_model.config['input_size'] = len(selected_features)
        
        # Fit a single StandardScaler on training data only to prevent data leakage
        X_train_selected, scaler = MLPDataUtils.scale_data(X_train_selected, None, True)
        # Remove rows with the highest 50 date_int values
        if 'date_int' in X_test_selected.columns:
            threshold = X_test_selected['date_int'].copy()
            threshold = threshold.drop_duplicates().max()-15
            logger.info(f"üìÖ Threshold: {threshold}")
            mask = X_test_selected['date_int'] < threshold
            X_test_selected, y_test = X_test_selected[mask], y_test[mask]
            logger.info(f"üìÖ Removed rows with date_int >= {threshold} (kept {len(X_test_selected)} samples)")
        else:
            logger.warning("‚ö†Ô∏è 'date_int' column not found - skipping date filtering")
        
        X_test_scaled, _ = MLPDataUtils.scale_data(X_test_selected, scaler, False)
        # Store the fitted scaler in the SAME model instance
        mlp_model.scaler = scaler
        
        # Create objective function using the SAME MLP model instance with selected features
        objective_function = mlp_model.objective(X_train_selected, y_train, X_test_selected, X_test_scaled, y_test, fitted_scaler=scaler)
        sampler = optuna.samplers.RandomSampler(seed=42)

        # Run optimization
        study = optuna.create_study(direction='maximize', sampler=sampler)
        study.optimize(objective_function, n_trials=number_of_trials, n_jobs=1)
        
        # Get best results from study
        best_params = study.best_params
        best_profit = study.best_value  # This is now threshold-optimized profit per investment
        
        logger.info("üéØ Hyperparameter optimization with threshold optimization completed!")
        logger.info(f"‚úÖ Best Threshold-Optimized Profit per Investment: ${best_profit:.2f}")
        logger.info(f"‚úÖ Best parameters: {best_params}")
        
        # Finalize the best model (ensure mlp_model contains the best performing model)
        mlp_model.finalize_best_model()
        
        # Get best trial info for verification (now includes threshold info)
        best_trial_info = mlp_model.get_best_trial_info()
        logger.info(f"‚úÖ Best trial info: {best_trial_info}")
        
        # Use the SAME instance throughout - no need to create final_model
        final_model = mlp_model

        # Extract current prices for evaluation
        final_current_prices = X_test_selected['close'].values

        # Evaluate with the optimal threshold from hyperparameter optimization
        optimal_threshold = getattr(final_model, 'optimal_threshold', 0.5)
        confidence_method = getattr(final_model, 'confidence_method', 'variance')
        
        threshold_performance = final_model.threshold_evaluator.evaluate_threshold_performance(
            model=final_model,
            X_test=X_test_selected,
            y_test=y_test,
            current_prices_test=final_current_prices,
            threshold=optimal_threshold,
            confidence_method=confidence_method
        )
        
        # Also get unfiltered baseline for comparison
        baseline_predictions = final_model.predict(X_test_selected)
        baseline_profit = final_model.threshold_evaluator.calculate_profit_score(
            y_test.values, baseline_predictions, final_current_prices
        )
        baseline_profit_per_investment = baseline_profit / len(y_test)
        
        logger.info("üìä Final Results Comparison:")
        logger.info(f"   Baseline (unfiltered) profit per investment: ${baseline_profit_per_investment:.2f}")
        logger.info(f"   Threshold-optimized profit per investment: ${threshold_performance['profit_per_investment']:.2f}")
        logger.info(f"   Improvement ratio: {threshold_performance['profit_per_investment'] / baseline_profit_per_investment if baseline_profit_per_investment != 0 else 0:.2f}x")
        logger.info(f"   Samples kept: {threshold_performance['samples_evaluated']}/{len(X_test_selected)} ({threshold_performance['samples_kept_ratio']:.1%})")
        logger.info(f"   Investment success rate: {threshold_performance['investment_success_rate']:.3f}")
        
        # Use threshold-optimized metrics for final evaluation
        final_profit_per_investment = threshold_performance['profit_per_investment']
        final_total_profit = threshold_performance['total_profit']
        final_investment_success_rate = threshold_performance['investment_success_rate']
        final_samples_kept = threshold_performance['samples_evaluated']
        
        # Traditional metrics on filtered data
        final_mse = threshold_performance['mse']
        final_mae = threshold_performance['mae']
        final_r2 = threshold_performance['r2_score']
        
        # Store threshold results for MLflow logging
        threshold_metrics = {
            'final_optimal_threshold': final_model.best_threshold_info['optimal_threshold'],
            'final_samples_kept_ratio': threshold_performance['samples_kept_ratio'],
            'final_investment_success_rate': final_investment_success_rate,
            'final_baseline_profit_per_investment': baseline_profit_per_investment,
            'final_improvement_ratio': threshold_performance['profit_per_investment'] / baseline_profit_per_investment if baseline_profit_per_investment != 0 else 0
        }
        
        logger.info("üìä Final Optimized Results:")
        logger.info(f"   Total Profit: ${final_total_profit:.2f}")
        logger.info(f"   Profit per Investment: ${final_profit_per_investment:.2f}")
        logger.info(f"   Samples Used: {final_samples_kept}/{len(X_test_selected)} (threshold-filtered)")
        logger.info(f"   Traditional MSE: {final_mse:.4f}")
        logger.info(f"   Traditional MAE: {final_mae:.4f}")
        logger.info(f"   Traditional R¬≤: {final_r2:.4f}")
        
        
        # Prepare comprehensive metrics for logging
        final_metrics = {
            "final_total_profit": final_total_profit,
            "final_profit_per_investment": final_profit_per_investment,
            "final_mse": final_mse,
            "final_mae": final_mae,
            "final_r2": final_r2,
            "final_test_samples": len(y_test),
            "hypertuning_best_profit": best_profit,
            "hypertuning_trials_completed": number_of_trials,
            "features_selected": len(selected_features)
        }
        
        # Add threshold optimization metrics if successful
        final_metrics.update(threshold_metrics)
        
        # Prepare comprehensive parameters for logging
        final_params = best_params.copy()
        final_params.update({
            "prediction_horizon": prediction_horizon,
            "hypertuning_trials": number_of_trials,
            "target_column": target_column,
            "split_date": data_result['split_date'],
            "feature_count": data_result['feature_count'],
            "train_samples": data_result['train_samples'],
            "test_samples": data_result['test_samples'],
            "threshold_method": final_model.confidence_method,
            "threshold_optimization_during_hypertuning": True,
            "optimal_threshold_from_hypertuning": final_model.best_threshold_info['optimal_threshold']
        })
        
        # Use the universal logging function via save_model method
        saved_run_id = final_model.save_model(
            metrics=final_metrics,
            params=final_params,
            X_eval=X_test_selected,
            experiment_name=experiment_name,
            scaler=scaler
        )
        
        logger.info(f"‚úÖ Model saved using updated save_model method. Run ID: {saved_run_id}")
        
        logger.info("=" * 80)
        logger.info("üéâ STANDALONE MLP HYPERTUNING COMPLETED SUCCESSFULLY!")
        logger.info("=" * 80)
        logger.info(f"üìä Dataset: {data_result['train_samples'] + data_result['test_samples']:,} samples, {data_result['feature_count']} features")
        logger.info(f"üéØ Target: {target_column} ({prediction_horizon}-day horizon)")
        logger.info(f"üìÖ Train period: {train_date_range}")
        logger.info(f"üìÖ Test period: {test_date_range}")
        logger.info(f"üîß Hypertuning: {number_of_trials} trials completed (optimizing for profit)")
        logger.info(f"üìà Final Total Profit: ${final_total_profit:.2f}")
        logger.info(f"üìà Average Profit per Investment: ${final_profit_per_investment:.2f}")
        logger.info(f"üìà Traditional MSE: {final_mse:.4f}")
        logger.info(f"üíæ Model saved to MLflow run: {saved_run_id}")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"‚ùå CRITICAL ERROR in standalone MLP hypertuning: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main() 